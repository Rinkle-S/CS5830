# -*- coding: utf-8 -*-
"""NA19B056_A05.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10ZlP6DACGAzOUZ_JzW962SResIiM6HUG

# Import libraries and dataset exploration
"""

pip install tensorflow

import numpy as np

pip install plot-keras-history

from tensorflow import keras
from keras import layers
from keras.layers import Dropout
from keras import regularizers
import matplotlib.pyplot as plt
from plot_keras_history import show_history

import warnings
warnings.filterwarnings('ignore')

pip install mlflow

!pip install mlflow
!pip install pyngrok

from pyngrok import ngrok

# Set your Ngrok auth token (sign up at https://ngrok.com to get your auth token)
!ngrok authtoken 2gV2O7WGty07bU0vYTqg52qMSTz_74yGnzPeBT5ZxXAF6reXR

ngrok_tunnel = ngrok.connect(5000)
public_url = ngrok_tunnel.public_url
print(f"MLflow tracking URI: {public_url}")

# Start the MLflow server
get_ipython().system_raw("mlflow ui --port 5000 &")

import mlflow

# Set the MLflow tracking URI to the public URL generated by Ngrok
mlflow.set_tracking_uri(public_url)
mlflow.set_experiment("mnist-classification")

(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()
num_classes = 10
x_train = X_train.reshape(60000, 784)
x_test = X_test.reshape(10000, 784)
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print(x_train.shape, 'train input samples')
print(x_test.shape, 'test input samples')

y_train = keras.utils.to_categorical(Y_train, num_classes)
y_test = keras.utils.to_categorical(Y_test, num_classes)
print(y_train.shape, 'train output samples')
print(y_test.shape, 'test output samples')

plt.subplot(221)
plt.imshow(X_train[310], cmap=plt.get_cmap('gray'))
plt.subplot(222)
plt.imshow(X_train[515], cmap=plt.get_cmap('gray'))
plt.subplot(223)
plt.imshow(X_train[1210], cmap=plt.get_cmap('gray'))
plt.subplot(224)
plt.imshow(X_train[2150], cmap=plt.get_cmap('gray'))
plt.show()

"""# Experiment version 1

## Without auto logging
"""

# basic neural network for digit classification
with mlflow.start_run(run_name='Basic NN',nested=True):
    # Getting explicit control over hyperparameters
    mlflow.log_param("num_nodes_input", 20)
    mlflow.log_param("num_nodes_hidden", 20)
    mlflow.log_param("num_nodes_output", 10)
    mlflow.log_param("activation", "sigmoid")

    model = keras.Sequential()
    model.add(layers.Dense(20, activation='sigmoid', input_shape=(784,)))
    model.add(layers.Dense(20, activation='sigmoid'))
    model.add(layers.Dense(10, activation='softmax'))
    model.compile(loss='categorical_crossentropy', metrics=['accuracy'])
    history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))

    for epoch in range(10):
        # Access metrics by key from the dictionary
        mlflow.log_metric("loss", history.history.get("loss")[epoch], step=epoch)
        mlflow.log_metric("val_loss", history.history.get("val_loss")[epoch], step=epoch)
        mlflow.log_metric("accuracy", history.history.get("accuracy")[epoch], step=epoch)
        mlflow.log_metric("val_accuracy", history.history.get("val_accuracy")[epoch], step=epoch)

"""## With auto logging"""

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

# Enable automatic logging
mlflow.tensorflow.autolog()

# Define the neural network parameters
input_shape = (784,)
layer1_units = 20
layer2_units = 20
output_units = 10
activation_function = 'sigmoid'
output_activation = 'softmax'
learning_rate = 0.001
optimizer = Adam(learning_rate=learning_rate)
loss_function = 'categorical_crossentropy'
metrics = ['accuracy']
epochs = 10

with mlflow.start_run(run_name='Auto Basic NN', nested=True):
    # Log the parameters manually
    mlflow.log_param("input_shape", input_shape)
    mlflow.log_param("layer1_units", layer1_units)
    mlflow.log_param("layer2_units", layer2_units)
    mlflow.log_param("output_units", output_units)
    mlflow.log_param("activation_function", activation_function)
    mlflow.log_param("output_activation", output_activation)
    mlflow.log_param("learning_rate", learning_rate)
    mlflow.log_param("optimizer", "Adam")
    mlflow.log_param("loss_function", loss_function)
    mlflow.log_param("metrics", metrics)
    mlflow.log_param("epochs", epochs)

    # Build and compile the model
    model = keras.Sequential()
    model.add(layers.Dense(layer1_units, activation=activation_function, input_shape=input_shape))
    model.add(layers.Dense(layer2_units, activation=activation_function))
    model.add(layers.Dense(output_units, activation=output_activation))
    model.compile(optimizer=optimizer, loss=loss_function, metrics=metrics)

    # Train the model
    history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))

show_history(history)

loss, acc = model.evaluate(x_test, y_test, verbose=2)
print("Test accuracy: {:5.2f}%".format(100*acc))
loss, acc = model.evaluate(x_train, y_train, verbose=2)
print("Train accuracy: {:5.2f}%".format(100*acc))

# check if the prediction is working fine for a random test point
test_pt = 782
plt.imshow(X_test[test_pt], cmap=plt.get_cmap('gray'))
probs = model.predict(x_test[test_pt:test_pt+1], verbose=True)
print("Predicted Digit:", np.argmax(probs))

"""# Experiment version 2"""

# slightly bigger model with more parameters.
with mlflow.start_run(run_name="Bigger NN",nested=True):
    model2 = keras.Sequential()
    model2.add(layers.Dense(256, activation='sigmoid', input_shape=(784,)))
    model2.add(layers.Dense(128, activation='sigmoid'))
    model2.add(layers.Dense(10, activation='softmax'))
    model2.compile(loss='categorical_crossentropy', metrics=['accuracy'])
    history = model2.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))

show_history(history)

loss, acc = model2.evaluate(x_test, y_test, verbose=2)
print("Test accuracy: {:5.2f}%".format(100*acc))
loss, acc = model2.evaluate(x_train, y_train, verbose=2)
print("Train accuracy: {:5.2f}%".format(100*acc))

"""# Experiment version 3"""

# adding kernel regularization to the mix.
with mlflow.start_run(run_name='Kernel-reg NN',nested=True):
    model_r = keras.Sequential()
    model_r.add(layers.Dense(256, activation='sigmoid', input_shape=(784,), kernel_regularizer=regularizers.L2(0.01)))
    model_r.add(layers.Dense(128, activation='sigmoid', kernel_regularizer=regularizers.L2(0.01)))
    model_r.add(layers.Dense(10, activation='softmax'))
    model_r.compile(loss='categorical_crossentropy', metrics=['accuracy'])
    history_r = model_r.fit(x_train, y_train, epochs=50, steps_per_epoch=50, validation_data=(x_test, y_test))

loss, acc = model_r.evaluate(x_test, y_test, verbose=2)
print("Test accuracy: {:5.2f}%".format(100*acc))
loss, acc = model_r.evaluate(x_train, y_train, verbose=2)
print("Train accuracy: {:5.2f}%".format(100*acc))

show_history(history_r)

"""# Experiment version 4"""

with mlflow.start_run(run_name='Dropout NN',nested=True):
    model_rd = keras.Sequential()
    model_rd.add(layers.Dense(256, activation='sigmoid', input_shape=(784,)))
    model_rd.add(Dropout(0.7))
    model_rd.add(layers.Dense(128, activation='sigmoid'))
    model_rd.add(Dropout(0.6))
    model_rd.add(layers.Dense(10, activation='softmax'))
    model_rd.compile(loss='categorical_crossentropy', metrics=['accuracy'])
    history_rd = model_rd.fit(x_train, y_train, epochs=10)

show_history(history_rd)

loss, acc = model_rd.evaluate(x_test, y_test, verbose=2)
print("Test accuracy: {:5.2f}%".format(100*acc))
loss, acc = model_rd.evaluate(x_train, y_train, verbose=2)
print("Train accuracy: {:5.2f}%".format(100*acc))

"""# Experiment version 5"""

# using early stopping method
with mlflow.start_run(run_name='Early stop NN',nested=True):
    model_re = keras.Sequential()
    model_re.add(layers.Dense(256, activation='sigmoid', input_shape=(784,)))
    model_re.add(layers.Dense(128, activation='sigmoid'))
    model_re.add(layers.Dense(10, activation='softmax'))
    model_re.compile(loss='categorical_crossentropy', metrics=['accuracy'])
    es = keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=2)
    history_es = model_re.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), callbacks=[es])

show_history(history_es)

"""# Experiment version 6"""

# Using LEarning rates now.
with mlflow.start_run(run_name='LR 0.1 NN'):
    model3 = keras.Sequential()
    model3.add(layers.Dense(20, activation='sigmoid', input_shape=(784,)))
    model3.add(layers.Dense(10, activation='sigmoid'))
    model3.add(layers.Dense(10, activation='softmax'))
    opt_new = keras.optimizers.SGD(learning_rate= 0.1)
    model3.compile(optimizer=opt_new, loss='categorical_crossentropy', metrics=['accuracy'])
    model3.fit(x_train, y_train, epochs=20)

"""# Experiment version 7"""

# too low learning rate
with mlflow.start_run(run_name='Low Lower NN'):
    model3 = keras.Sequential()
    model3.add(layers.Dense(20, activation='sigmoid', input_shape=(784,)))
    model3.add(layers.Dense(10, activation='sigmoid'))
    model3.add(layers.Dense(10, activation='softmax'))
    model3.summary()
    opt_new = keras.optimizers.SGD(learning_rate=.0001)
    model3.compile(optimizer=opt_new, loss='categorical_crossentropy', metrics=['accuracy'])
    model3.fit(x_train, y_train, epochs=20)

"""# Experiment version 8"""

# optimal learning rate
with mlflow.start_run(run_name='Optimal Lower NN'):
    model3 = keras.Sequential()
    model3.add(layers.Dense(20, activation='sigmoid', input_shape=(784,)))
    model3.add(layers.Dense(10, activation='sigmoid'))
    model3.add(layers.Dense(10, activation='softmax'))
    model3.summary()
    opt_new = keras.optimizers.SGD(learning_rate=.01)
    model3.compile(optimizer=opt_new, loss='categorical_crossentropy', metrics=['accuracy'])
    model3.fit(x_train, y_train, epochs=20)

"""# Experiment version 9"""

# optimal learning rate with momentum
with mlflow.start_run(run_name='Opt LR & Momentum NN'):
    model3 = keras.Sequential()
    model3.add(layers.Dense(20, activation='sigmoid', input_shape=(784,)))
    model3.add(layers.Dense(10, activation='sigmoid'))
    model3.add(layers.Dense(10, activation='softmax'))
    model3.summary()
    opt_new = keras.optimizers.SGD(learning_rate=.01, momentum=0.5)
    model3.compile(optimizer=opt_new, loss='categorical_crossentropy', metrics=['accuracy'])
    model3.fit(x_train, y_train, epochs=20)

"""# Experiment version 10"""

# Mini-batch SGD
with mlflow.start_run(run_name='Mini-batch SGD NN'):  # the default minibatch size is 32 unlike 1.
    model4 = keras.Sequential()
    model4.add(layers.Dense(20, activation='sigmoid', input_shape=(784,)))
    model4.add(layers.Dense(10, activation='sigmoid'))
    model4.add(layers.Dense(10, activation='softmax'))
    model4.summary()
    opt_new = keras.optimizers.SGD(learning_rate=.01, momentum=0.5)
    model4.compile(loss='categorical_crossentropy', metrics=['accuracy'])
    model4.fit(x_train, y_train, batch_size=512, epochs=10)